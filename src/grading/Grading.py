
import re
from warnings import warn
from dotmap import DotMap
from copy import deepcopy
from src.utils.utils import format_string
from src.agent.ChatGPT import ChatGPT

class Grading():

    def __init__(self, config, test_run=False) -> None:
        self.name = "grading_test_run" if test_run else "grading"
        self.config = config 
        self.test_run = test_run
        self.agent = ChatGPT(DotMap(self.config.task.evaluation.agent), seed=self.config.seed)

    def _generate(self, df, gen_param):
        return (df.groupby("id", as_index=False, group_keys=False)
                .apply(self.create_zero_shot_messages, 
                        gen_param=gen_param))

    def create_zero_shot_messages(self, row, gen_param):
        instructions = self.config.task.evaluation.toDict()["instructions"]
        # format the instructions with the generated information 
        # and makes sure that the format is appropriate 
        instr = deepcopy(instructions)
        instr[1]["content"] = (instr[1]["content"] 
                            + "\n" 
                            + format_string(row.iloc[0].to_dict(), instr[2]["content"]))
        instr = instr[:-1]

        # gathering all information from the judge grading
        prompt = "\n".join([f"{m['role']}: {m['content']}" for m in instr])

        outputs = self.agent.query_with_message(instr, **gen_param)  # still using instr
        response = outputs[0] # assuming a single response generated
        beacons = list(self.config.task.evaluation.beacons)
        answers = match_criteria(beacons, response)
        
        # if no answer are provided, the answer is automatically wrong
        if None in answers:
            answers = [False]
            warn("No answers generated by judge in non-evaluation mode")
        
        # list of grading criteria values 
        row["judge_answer"] =  "-".join([str(a) for a in answers])
        criteria = list(self.config.task.evaluation.criteria)
        for c, a in zip(criteria, answers):
            row[c] = a 
            
        row["judge_full_prompt"] =  prompt

        return row 


def match_criteria(beacons, response):
    results = []
    for beacon in beacons:
        # TODO: need to handle the special case where no answer is provided
        # in that case simply return special value which will be mapped
        # to the opposite of what is expected 
        if beacon not in response:
            warn(f"Beacon {beacon} not in response: {response}, returning false")
            results.append(False)
            continue

        search_str = response[response.index(beacon) + len(beacon):]
        match_yes = bool(re.match("[^a-zA-Z\d]*Yes", search_str))
        match_no = bool(re.match("[^a-zA-Z\d]*No", search_str))
        if (not match_yes and not match_no) or (match_yes and match_no):
            warn(f"Model answer does not match expectation {response}, returning false")
            results.append(False)
            continue

        results.append(match_yes)

    return results


